Abstract: From both human translators (HT) and machine translation (MT) researchersÃÂ¢ÃÂÃÂ point of view, translation quality evaluation (TQE) is an essential task. Translation service providers (TSPs) have to deliver large volumes of translations which meet customer specifications with harsh constraints of required quality level in tight time-frames and costs. MT researchers strive to make their models better, which also requires reliable quality evaluation. While automatic machine translation evaluation (MTE) metrics and quality estimation (QE) tools are widely available and easy to access, existing automated tools are not good enough, and human assessment from professional translators (HAP) are often chosen as the golden standard (Han et al., 2021b). Human evaluations, however, are often accused of having low reliability and agreement. Is this caused by subjectivity or statistics is at play? How to avoid the entire text to be checked and be more efficient with TQE from cost and efficiency perspectives, and what is the optimal sample size of the translated text, so as to reliably estimate the translation quality of the entire material? This work carries out such a motivated research to correctly estimate the confidence intervals (Brown et al., 2001) depending on the sample size of translated text, e.g. the amount of words or sentences, that needs to be processed on TQE workflow step for confident and reliable evaluation of overall translation quality. The methodology we applied for this work is from Bernoulli Statistical Distribution Modelling (BSDM) and Monte Carlo Sampling Analysis (MCSA). Keywords: Translation Quality Evaluation, Quality Estimation, Post-editing Distance, Confidence Intervals, Monte Carlo Modeling, Bernoulli Statistics Introduction Machine Translation (MT) is one of the pioneering artificial intelligence (AI) tasks dating back from the 1950s (Weaver, 1955). It emphasizes the interaction of Language and Machine, and how machine can learn human languages with cognitive knowledge. Before MT, human translation (HT) of written text and documents has always played an important role in science and literature communication between different language speakers, breaking the language barriers. From both MT and HT perspectives, trans- lation quality evaluation (TQE), sometimes incorrectly referred as translation quality assessment (TQA)i, is an important task to reflect how well the source text is translated into the target languages (Han et al., 2021b). On the one hand, for low resource language pair scenarios, human translators still play the dom- inant role in translation production. The translated text and documents can contain unavoidable errors due to personal bias, input efforts, or the training level of the translators. On the other hand, for high resource language pair situation, neural MT (NMT) has achieved remarkable improvement especially on fluency level, compared to conventional rule-based and statistical phrase-based MT models; however, NMT still has Ã¢ÂÂpoisoned cookieÃ¢ÂÂ problem struggling to achieve real human parity, for instance, on ad- equacy level, meaning preservation, and on idiomatic expression translations (Sag et al., 2002; Han et al., 2020b; Johnson et al., 2016; Han et al., 2020a). Translation service providers (TSPs) relying on both MT, HT, and human post-editing of MT output (TPE) carry out translation and editing tasks with the high demand and harsh constraints nowadays. Thus, TQE role in this workflow remains to be critical. However, it is tedious, costly, and time-consuming to check through the entire translated text given the huge amount of data TSP and customers process. One obvious solution, to this point, is to extract a sub-set of the translated text and make a conclusion about the overall translation quality by results of TQE of the sample, which has always been done in real practice. However, one question arises here: how large the sample size shall be to estimate the overall translation quality of the entire material reliably? In other words, what is the confidence interval of such evaluation on certain desired confidence level (which is commonly taken as 95%) with the samples we choose to estimate the overall translation quality? In this work, we carry out such a motivated experimental investigation on confidence evaluation of translation quality evaluation. To take advantage of statistical modeling techniques, we start with the assumption that error distribution is uniform across the entire material, and minimum unit where the error occurs is one sentence (since errors can be between words, in the form of punctuation and conjugation, etc.). This assumption is the best case scenario which potentially ensures the smallest size of the sample, because if it is not correct we immediately arrive to situation whenneed to check more, if not entire text. We start from high quality translation assumption that errors are rare (average error density is as low as 0.07 errors per sentence). Then, on the second stage of experiment, by taking post-editing distance (PED) measurement for evaluation, we prove that such assumption can be applied to the situation where errors are much more frequent, with large number of Ã¢ÂÂerrorsÃ¢ÂÂ (edits) per one sentence. We use Bernoulli Statistical Distribution Modeling (BSDM) and Monte Carlo Sampling Analysis (MCSA) to explore the confident level sample size estimation. With this methodology, we expect to reach a conclusion where practical suggestion can be given on confident sample size of translated text/document quality evaluation. To our knowledge, this is the first study to carry out statistical confidence intervals estimation for MT quality assessment, while in all practical situations, researchers and practitioners very often take it for granted by randomly choosing a sample size from MT outputs to estimate the system quality. This paper is organized as below: Section 1 presents the topic of this work, Section 2 covers some related work to our study, Section 3 introduces the statistical modelling, Section 4 presents the Monte Carlo (MC) simulation, Section 5 carries out confidence estimation for post-editing distance metric using MC simulation, and finally Section 6 concludes the work with discussions and future work. Related Work Quality estimation and measurement have been explored using various strategies for both human and machine translated text. For instance, human evaluation criteria have included intelligibility, fidelity, flu- ency, adequacy, and comprehension as earlier stage development. These are often carried out based on both source text and candidate translations. Then, editing distance, or post-editing distance are applied to such study by calculating editing steps that are needed to revise the candidate translations towards a correct text sometimes using reference translations. Whenever reference translations are available, au- tomatic reference-based evaluation of the candidate translation quality can be measured via many com- putational metrics including n-gram based simple word matching as well as some linguistic motivated features such as paraphrasing and Part-of-Speech tagging (Han et al., 2021b). For automatic post-editing distance based metrics, algorithms are designed to calculate the editing steps of insertion, deletion, sub- stitution and/or word reordering. While automatic metrics have been getting very popular due to their low cost and repeatability, and thus easier to perform comparisons to previously published work, the credibility of such automatic evaluations have always been an issue. For instance, in contrast to the findings from conversational statistical MT researchers (Sanchez-Torron and Koehn, 2016), very recent research work on Neural MT (MT) by (Zouhar et al., 2021; Freitag et al., 2021; Han et al., 2021a) has shown that automatic evaluation metrics such as BLEU do not agree well with professional translators or experts based human evaluations, instead they tend to correlate closely to lower-quality crowd-source based human evaluation. Another disadvantage for automatic evaluation is that we can not get in-depth view of what kind of errors the candidate translations present in the studied context, except for an overall evaluation score, or segment-level scores, not to mention that most metrics do not even allow for clear interpretation of what does the score exactly mean. Regarding this aspect, professional translators can always do a much better job by giving transparent error analysis and categorization on the candidate translations, such as idiomatic expressions (Han et al., 2020a; Han et al., 2020b). However, another issue arrives at this point, that is how to correctly chose a confident sample from the candidate translation, instead of just take a random sample for granted and try to blindly extrapolate its result to the entire material? Actually this is not a brand new challenge in natural language processing (NLP) field. Having it in mind that randomly chosen samples may contain model bias against a proper evaluation, (Prabhu et al., 2019) proposed an uncertainty sampling approach for text classification task, and their statistical models can reduce the bias effectively with smaller size of data in comparison to confessional models; Similarly, (Haertel et al., 2008) carried out work on how statistical sampling models can help reduce the high cost for Penn Tree-bank annotation while maintaining the higher accuracy; (Nadeem et al., 2020) carried out one systematic comparison of several sampling algorithms used for language generation task, including top-k, nucleus and tempered sampling, looking into quality-diversity trade-off. Sampling method was also applied into confident level evaluation of MT. For instance, (Koehn, 2004)to use bootstrap re-sampling methods to test the significance level of automatic metric BLEU, but using a fixed number of sentences, i.e. 300. Like many other research work, the chosen number of sentences for evaluation was never explained or justified with any statistical validation. In contrast, we carry out statistical validation by taking post-editing distance (PED) measurement for evaluation, and by taking post-editing distance (PED) value-based distance modeling techniques for detail. The results of this work are published as supporting information (TPE). Related Work Quality estimation and measurement have been explored using various strategies for both human and machine translated text. For instance, human evaluation criteria have included intelligibility, fidelity, flu- ency, adequacy, and comprehension as earlier stage development. These are often carried out based on both source text and candidate translations. Then, editing distance, or post-editing distance are applied to such study by calculating editing steps that are needed to revise the candidate translations towards a correct text sometimes using reference translations. Whenever reference translations are available, au- tomatic reference-based evaluation of the candidate translation quality can be measured via many com- putational metrics including n-gram based simple word matching as well as some linguistic motivated features such as paraphrasing and Part-of-Speech tagging (Han et al., 2021b). For automatic post-editing distance based metrics, algorithms are designed to calculate the editing steps of insertion, deletion, sub- stitution and/or word reordering. While automatic metrics have been getting very popular due to their low cost and repeatability, and thus easier to perform comparisons to previously published work, the credibility of such automatic evaluations have always been an issue. For instance, in contrast to the findings from conversational statistical MT researchers (Sanchez-Torron and Koehn, 2016), very recent research work on Neural MT (MT) by (Zouhar et al., 2021; Freitag et al., 2021; Han et al., 2021a) has shown that automatic evaluation metrics such as BLEU do not agree well with professional translators or experts based human evaluations, instead they tend to correlate closely to lower-quality crowd-source based human evaluation.
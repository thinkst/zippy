Abstract Recent progress in pretrained Transformerbased language models has shown great success in learning contextual representation of text. However, due to the quadratic selfattention complexity, most of the pretrained Transformers models can only handle relatively short text. It is still a challenge when it comes to modeling very long documents. In this work, we propose to use a graph attention network on top of the available pretrained Transformers model to learn document embeddings. This graph attention network allows us to leverage the high-level semantic structure of the document. In addition, based on our graph document model, we design a simple contrastive learning strategy to pretrain our models on a large amount of unlabeled corpus. Empirically, we demonstrate the effectiveness of our approaches in document classification and document retrieval tasks. Introduction Document representations that capture the semantics are crucial to various document-level Natural Language Processing (NLP) tasks, including sentiment analysis (Medhat et al., 2014), text classification (Kowsari et al., 2019) and information retrieval (Lin et al., 2020). In recent years, an increasing volume of work has focused on learning a task-agnostic universal representation for long documents. While improved performance in downstream tasks have been achieved, there are two challenges towards learning a high quality document representation: (1) absence of document structure. Most works treat the document as a sequence of tokens without considering high-level structure. (2) data scarcity. Existing methods in document representation learning are significantly affected by the scarcity of document-level data. Transformers-based pretrained language models are ubiquitously state-of-the-art across many NLP tasks. Transformer models such as BERT (Devlin et al., 2019) and its variants have shown great success in learning contextual representation of text. Representation from large language models can partially mitigate the data scarcity issue due to pretraining on a large amounts of unlabeled data. However, those models mostly consider token-level information and their pretraining tasks are not directly targeting long document representations. Another issue of directly applying transformer-based models is the limit of the input text length. Due to the quadratic complexity of self-attention, most of the pretrained transformers models can only handle a relatively short text. A wide spectrum of efficient, fast transformer models (collectively called Ã¢ÂÂXformersÃ¢ÂÂ) have been proposed to tackle this problem; e.g., Longformer (Beltagy et al., 2020) and Bigbird (Zaheer et al., 2020) use sparse attention to improve the computational and memory efficiency for long sequence text. Nevertheless, these models still focus on token-level interactions without considering high-level semantic structure of the document. Recently, there is a resurgence of interest in Contrastive Learning (CL) due to its success in selfsupervised representation learning in computer vision (Chen et al., 2020; He et al., 2020). Contrastive Learning offers a simple method to learn disentangled representation that encodes invariance to small and local changes in the input data without using any labeled data. In NLP domain, contrastive learning has been employed to learn sentence representation (Wu et al., 2020; Qu et al., 2020) under either self-supervised or supervised settings. In this work, we propose a Graph Attention Network (GAT) based model that explicitly utilizes the high-level semantic structure of the documents to learn document embeddings. We model the document as not just a sequence of text, but a collection of passages or sentences. Specifically, the proposed model introduces a graph on top of the document passages (Fig. 1) to utilize multi-granularity information. First, passages are encoded using RoBERTa (Liu et al., 2019) to collect word-level knowledge. Then passages are connected to leverage the higher-level structured information. At last, a graph attention network (Velickovi ÃÂ c et al. ÃÂ´ , 2017) is applied to obtain the multi-granularity document representation. To better learn the document embedding, we propose a document-level contrastive learning strategy to pretrain our models. In our contrastive learning framework, we split the document into random sub-documents and train the model to maximize the agreement over the representations of the sub-documents that come from the same document. This simple strategy allows us to pretrain our models on a large unlabelled corpus without any additional priors. As we will see, this simple pretraining task indeed helps the model on the downstream tasks. The contributions of this paper can be summarized as follows. Ã¢ÂÂ¢ We propose a graph document modelgraph attention networks that can not only explicitly utilize the high-level structure of the document but also leverage pretrained Transformer encoders to obtain low-level contextual information. ÃÂ¢ÃÂÃÂ¢ We propose a simple document-level contrastive learning strategy, which does not require any handcrafted transformations and is suitable for large-scale pretraining. ÃÂ¢ÃÂÃÂ¢ We conduct empirical evaluations on our approaches in document classification and document retrieval tasks. Introduction Document representations that capture the semantics are crucial to various document-level Natural Language Processing (NLP) tasks, including sentiment analysis (Medhat et al., 2014), text classification (Kowsari et al., 2019) and information retrieval (Lin et al., 2020). In recent years, an increasing volume of work has focused on learning a task-agnostic universal representation for long documents. While improved performance in downstream tasks have been achieved, there are two challenges towards learning a high quality document representation: (1) absence of document structure. Most works treat the document as a sequence of tokens without considering high-level structure. (2) data scarcity. Existing methods in document representation learning are significantly affected by the scarcity of document-level data. Transformers-based pretrained language models are ubiquitously state-of-the-art across many NLP tasks. Transformer models such as BERT (Devlin et al., 2019) and its variants have shown great success in learning contextual representation of text.
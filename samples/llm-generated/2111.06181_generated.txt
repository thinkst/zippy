Abstract Virtual Adversarial Training (VAT) has been effective in learning robust models under su- pervised and semi-supervised settings for both computer vision and NLP tasks. However, the efficacy of VAT for multilingual and multilabel text classification has not been explored before. In this work, we explore VAT for multilabel emotion recognition with a focus on leverag- ing unlabelled data from different languages to improve the model performance. We perform extensive semi-supervised experiments on Se- mEval2018 multilabel and multilingual emo- tion recognition dataset and show performance gains of 6.2% (Arabic), 3.8% (Spanish) and 1.8% (English) over supervised learning with same amount of labelled data (10% of training data). We also improve the existing state-of- the-art by 7%, 4.5% and 1% (Jaccard Index) for Spanish, Arabic and English respectively and perform probing experiments for under- standing the impact of different layers of the contextual models. Introduction Emotion recognition is an active and crucial area of research, especially for social media platforms. Un- derstanding the emotional state of the users from textual data forms an important problem as it helps in discovering signs of fear, anxiety, bullying, ha- tred etc. and maintaining the emotional health of the people and platform. With the advent of deep neural networks and contextual models, text under- standing has advanced dramatically by leveraging huge amount of unlabelled data freely available on web. However, even with these advancements, an- notating emotion categories is expensive and time consuming as emotion categories are highly cor- related and subjective in nature and can co-occur in the same text. Psychological studies suggest that emotions like "anger" and "sadness" are co- related and co-occur more frequently than "anger" and "happiness" (Plutchik, 1980). In a multilingual setup, the annotation becomes even more challeng- ing as annotator team are expected to be familiar with different languages and culture for understand- ing the emotions accurately. Imbalance in availabil- ity of the data across languages further creates prob- lems, especially in case of resource impoverished languages. In this work, we investigate the follow- ing key points; a) Can unlabelled data from other languages improve recognition performance of tar- get language and help in reducing requirement of labelled data? b) Efficacy of VAT for multilingual and multilabel setup. To address the aforementioned questions, we fo- cus our experiments towards semi-supervised learn- ing in a multilingual and multilabel emotion classi- fication framework. We formulate semi-supervised Virtual Adversarial Training (VAT) (Miyato et al.,2018) for multilabel emotion classification using contextual models and perform extensive exper- iments to demonstrate that unlabelled data from other languages Lul = {L1, L2, . . . , Ln} improves the classification on the target language Ltgt. We obtain competitive performance by reducing the amount of annotated data demonstrating cross- language learning. To effectively leverage the multilingual content, we use multilingual contex- tual models for representing the text across lan- guages. We also evaluate monolingual contextual models to understand the performance differences between multilingual and monolingual models and explore the advantages of domain-adaptive and task-adaptive pretraining of models for our task and observe substantial gains. We perform extensive experiments on the SemEval2018 (Affect in Tweets: Task E-c1) dataset (Mohammad et al., 2018) which contains tweets from Twitter annotated with 11 emotion cat- egories across three languages - English, Spanish and Arabic and demonstrate the effectiveness of semi-supervised learning across languages. To the best of our knowledge, our study is the first one to explore semi-supervised adversarial learning across different languages for multilabel classification. In summary, the main contributions of our work are the following: We explore Virtual Adversarial Training (VAT) for semi-supervised multilabel classifi- cation on multilingual corpus. Experiments demonstrating 6.2%, 3.8% and 1.8% improvements (Jaccard Index) on Ara- bic, Spanish and English by leveraging un- labelled data of other languages while using 10% of labelled samples. Improve state-of-the-art of multilabel emotion recognition by 7%, 4.5% and 1% (Jaccard Index) for Spanish,and English respec- tively. Experiments showcasing the advantages of domain-adaptive and task-adaptive pre- training. Related Work Semi-supervised learning is an important paradigm for tackling the scarcity of labelled data as it marries the advantages of supervised and un- supervised learning by leveraging the informa- tion hidden in large amount of unlabelled data along with small amount of labelled data (Yang et al., 2021), (Van Engelen and Hoos, 2020). Early approaches used self-training for leveraging the modelÃ¢ÂÂs own predictions on unlabelled data to obtain additional information during training (Yarowsky,1995)(McCloskyetal.,2006). Clark et al. (2018) proposed cross-view training (CVT) for various tasks like chunking, dependency pars- ing, machine translation and reported state-of-the- art results. CVT forces the model to make consis- tent predictions when using the full input or partial input. Ladder networks (Laine and Aila, 2016), Mean Teacher networks (Tarvainen and Valpola, 2017) are another way for semi-supervised learn- ing where temporal and model-weights are ensem- bled. Another popular direction towards semi- supervised learning is adversarial training where the data point is perturbed with random or care- fully tuned perturbations to create an adversarial sample. The model is then encouraged to main- tain consistent predictions for the original sample and the adversarial sample. Adversarial training was initially explored for developing secure and robust models (Goodfellow et al., 2014), (Xiao et al., 2018), (Saadatpanah et al., 2020) to pre- vent attacks. Miyato et al. (2016), Cheng et al. (2019), Zhu et al. (2019) showed that adversarial training can improve both robustness and gener- alization for classification tasks, machine transla- tion and GLUE benchmark respectively. Miyato et al. (2016), Sachan et al. (2019), Miyato et al. (2018) then applied the adversarial training for semi-supervised image and text classification show- ing substantial improvements. Emotion recognition is an important problem and has received lot of attention from the com- munity (Yadollahi et al., 2017), (Sailunaz et al., 2018). The taxonomies of emotions suggested by Plutchik wheel of emotions (Plutchik, 1980) and (Ekman, 1984) have been used by the majority of the previous work in emotion recognition. Emo- tion recognition can be formulated as a multiclass problem (Scherer and Wallbott, 1994), (Moham- mad, 2012) or a multilabel problem (Mohammad et al., 2018), (Demszky et al., 2020). In the multi- class formulation, the objective is to identify the presence of one of the emotion from the taxonomy whereas in a multilabel setting, more than one emo- tion can be present in the text instance. Binary relevance approach (Godbole and Sarawagi, 2004) is another way to break multilabel problem into multiple binary classification problems. However, this approach does not model the co-relation be- tween emotions. Seq2Seq approaches (Yang et al., 2018), (Huang et al., 2021) solve this problem by modelling the relationship between emotions by inferring emotion in an incremental manner. An interesting direction for handling data scarcity in emotion recognition is to use distant supervision by exploiting emojis (Felbo et al., 2017), hash- tags (Mohammad, 2012) or pretraining emotion specific embeddings and language models (Barbi- eri et al., 2021), (Ghosh et al., 2017). With the emergence of contextual models like BERT (Devlin et al., 2018), Roberta (Liu et al., 2019) etc., the field of NLP and text classifi- cation has been revolutionized as these models are able to learn efficient representations from a huge corpus of unlabelled data across different lan- guages and domains (Hassan et al., 2021), (Bar- bieri et al., 2021). Social media content contains linguistic errors, idiosyncratic styles, spelling mis- takes, grammatical inconsistency, slangs, hashtags, emoticons etc. (Barbieri et al., 2018), (Derczynski et al., 2013) due to which off-the-shelfmodels may not be optimum. We use language- adaptive, domain-adaptive and task-adaptive pre- training which has shown performance gains (Pe- ters et al., 2019), (Gururangan et al., 2020), (Barbi- eri et al., 2021), (Howard and Ruder, 2018), (Lee et al., 2020) for different tasks and domains. Conclusion In this work, we explore semi-supervised learning across different languages for multilabel classification. We perform extensive experiments on the SemEval2018 (Affect in Tweets: Task E-c1) dataset (Mohammad et al., 2018) which contains tweets from Twitter annotated with 11 emotion cat- egories across three languages - English, Spanish and Arabic and demonstrate the effectiveness of semi-supervised learning across languages. To the best of our knowledge, our study is the first one to explore semi-supervised adversarial learning across different languages for multilabel classification. 
Abstract Multi-task learning is useful in NLP because it is often practically desirable to have a single model that works across a range of tasks. In the medical domain, sequential training on tasks may sometimes be the only way to train models, either because access to the original (potentially sensitive) data is no longer available, or simply owing to the computational costs inherent to joint retraining. A major issue inherent to sequential learning, however, is catastrophic forgetting, i.e., a substantial drop in accuracy on prior tasks when a model is updated for a new task. Elastic Weight Consolidation is a recently proposed method to address this issue, but scaling this approach to the modern large models used in practice requires making strong independence assumptions about model parameters, limiting its effectiveness. In this work, we apply Kronecker FactorizationÃ¢ÂÂa recent approach that relaxes independence assumptionsÃ¢ÂÂto prevent catastrophic forgetting in convolutional and Transformer-based neural networks at scale. We show the effectiveness of this technique on the important and illustrative task of medical entity linking across three datasets, demonstrating the capability of the technique to be used to make efficient updates to existing methods as new medical data becomes available. On average, the proposed method reduces catastrophic forgetting by 51% when using a BERT-based model, compared to a 27% reduction using standard Elastic Weight Consolidation, while maintaining spatial complexity proportional to the number of model parameters. Introduction Creating a single model that performs well across multiple domains is often desirable, especially in production systems. Relying on multiple (task-specific) systems necessitates storing and managing corresponding collections of parameters. Multi-task models Caruana [1997] obviate this need by performing well on inputs from all tasks, simplifying deployment. In the medical domain especially, new data is constantly becoming available, and it is necessary to keep models up to date with this deluge. In medical entity linkingÃ¢ÂÂwhere the goal is to link mentions in clinical text to corresponding entities in an ontologyÃ¢ÂÂthe underlying ontologies are frequently updated, and the new terms are put into use quickly. For example, in the past year or so many codes related to COVID-19 Guan et al. [2020] were added to the International Classification of Diseases (ICD) lexicon, therefore updating models to incorporate new codes without losing performance on older knowledge is of particular importance. The language in the medical domain also brings additional challenges because of the many acronyms, synonyms, and ambiguous terms used in both clinical and biomedical corpora. These characteristics make it difficult even for humans to choose the correct entity among the top candidates in an ontology. To train a multi-task model, one would ideally jointly train on data drawn from all tasks. However, when new tasks are introduced, this would require re-training on the combined data, which is inefficient and sometimes practically impossible. For example, there are casesÃ¢ÂÂparticularly when dealing with medical dataÃ¢ÂÂwhere data access is lost, precluding joint training over a combined set of old and new data. This occurs, e.g., when license agreements or contracts with data providers expire; time-limited data partnerships are common in industry settings. In these situations, continuous learning, or training on task-specific data sequentially, is the only option to maintain a single model across tasks. Previous work on continuous learning has focused on mitigating catastrophic forgetting (CF) Mc- Closkey and Cohen [1989], Ratcliff [1990], a problem that arises in sequential training where performance on earlier tasks drops when the model is trained on additional tasks. Experience Replayde Masson dÃ¢ÂÂ Autume et al. [2019] maintains performance when training on new tasks by Ã¢ÂÂreplayingÃ¢ÂÂ examples saved from older tasks. Unfortunately, in a setting in which access to previous data is not possible, this method cannot be used. Elastic Weight Consolidation (EWC) Kirkpatrick et al. [2017] is an alternative, constraint-based technique that regularizes parameters such that they are encouraged to maintain optimal weights learned for prior tasks. By placing a prior involving the Hessian from previous tasks over network parameters, EWC affords flexibility with respect to changing parameters in different dimensions. Critically, EWC does not require continued access to data from Ã¢ÂÂpastÃ¢ÂÂ tasks once key statistics are computed over a taskÃ¢ÂÂs data. However, to scale to even relatively small neural networks, EWC must assume independence between all parameters. This assumption allows one to drop off-diagonal terms in the Fisher Information Matrix (FIM), which is used to approximate the Hessian; calculating the full matrix would be intractable. Recent work Ritter et al. [2018b] has proposed Kronecker(KF) ÃÂ¢ÃÂÃÂ which the optimization community uses to compute Hessians in neural networks ÃÂ¢ÃÂÃÂ to perform a version of EWC with a relaxed independence assumption on networks of linear layers operating on small vision datasets. Our contribution here is the extension of EWC and KF to the large-scale neural models now common in NLP. In particular, modern NLP tends to rely on large-scale models with hundreds of millions of parameters Devlin et al. [2019], Liu et al. [2019], Radford et al. [2019], and CF is problematic across many of its sub-domains. Though EWC has been successfully applied to NLP in recent work (Section 7), we demonstrate that there is room for substantial gains. In particular, we have observed that the independence assumption over parameters significantly and negatively affects EWCÃ¢ÂÂs ability to mitigate CF, as compared to what is achieved using the full covariance matrix of parameters. As far as we are aware, this work is the first application of the Kronecker Factorization methodÃ¢ÂÂ which relaxes the assumption of independence between parametersÃ¢ÂÂfor continuous learning in large-scale networks. Though we do not model all elements of the Fisher Information Matrix, we approximate block diagonals corresponding to layers, which is less damaging than assuming completely independent parameters. Specifically, we apply Kronecker Factorization in two large-scale neural networks for Entity Linking: (1) a convolutional and (2) a transformer-based architecture Vaswani et al. [2017]. Our primary contributions are as follows: (1) We combine and extend prior work in EWC and Kronecker Factorization to modern large-scale NLP models. (2) We use Kronecker Factorization to train on multiple biomedical ontology entity linking tasks sequentially without access to previous data, and show that it significantly outperforms baseline methods. Related Work CF Mitigation in Continuous Learning. There is a large body of work on CF mitigation, much of which focuses on constraining model parameters during training. From this category, we use learning rate control from UMLFit Howard and Ruder [2018] and EWC Kirkpatrick et al. [2017] as baselines. Path Integral Zenke et al. [2017] is similar to EWC but calculates weight variances from the optimization path instead of at the end of training. Chaudhry et al. [2018] combines these methods and generalizes the fisher by replacing it with a KL divergence between output distributions of the previous task weights and those of the current weights. Progressive networks Rusu et al. [2016] do not constrain training but use features of previously trained networks as additional input to subsequently trained ones. Progress and Compress Schwarz et al. [2018] extends this by using EWC to create one model that performs well on all tasks. Previous work has also focused on preventing CF by introducing a working memory that allows past examples to be replayed. We use experience replay from de Masson dÃ¢ÂÂ Autume et al. [2019] as an additional data-dependent baseline for our methods, but there are many other methods in this category Sprechmann et al. [2018], Wang et al. [2019], and many which combine these methods with regularization methods Lopez-Paz and Ranzato [2017], Chaudhry et al. [2019]. Kronecker Factorization for Neural Networks. Martens and Grosse [2015] first introduce Kro- necker Factorization as an approximation of blocks of the FIM of neural networks. They use this to perform second-order optimization techniques on linear neural networks, and then extend this to opti- mizing convolutional architectures Grosse and Martens [2016]. More recently, Ritter et al. [2018a] show that the Fisher approximation can be used as a posterior on network weights, and then expand EWC to use off-diagonal elements of the FIM in its regularization term with this approximation Ritter et al. [2018b]. This last paper mainly focuses on small vision datasets. Continuous Learning in NLP. NLP seems to be particularly susceptible to CF Howard and Ruder [2018], Yogatama et al. [2019]. Recent work has therefore focused on developing continuous learning techniques in NLP to mitigate this issue Moeed et al. [2020], Pilault et al. [2020], Chen et al. [2020]. There has also been work that applies previous techniques to specific domains (e.g. machine translation Thompson et al. [2019], sentiment analysis Madasu and Rao [2020], and reading comprehension Xu et[2020]). Many of these methods focus on avoiding over-fitting to new tasks during fine-tuning, whereas we focus on maintaining high performance on old tasks. None of these use Kronecker Factorization, which has not yet been scaled to prevent CF in large NLP models. CF mitigation is particularly important in clinical NLP given that many clinical datasets are quite different from generic domains and from each other quickly. For example, there are casesÃ¢ÂÂparticularly when dealing with medical dataÃ¢ÂÂwhere data access is lost, precluding joint training over a combined set of old and new data. This occurs, e.g., when license agreements or contracts with data providers expire; time-limited data partnerships are common in industry settings. In these situations, continuous learning, or training on task-specific data sequentially, is the only option to maintain a single model across tasks. Previous work on continuous learning has focused on mitigating catastrophic forgetting (CF) Mc- Closkey and Cohen [1989], Ratcliff [1990], a problem that arises in sequential training where performance on earlier tasks drops when the model is trained on additional tasks. Experience Replayde Masson dÃ¢ÂÂ Autume et al. [2019] maintains performance when training on new tasks by Ã¢ÂÂreplayingÃ¢ÂÂ examples saved from older tasks. Unfortunately, in a setting in which access to previous data is not possible, this method cannot be used.
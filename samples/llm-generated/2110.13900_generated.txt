Abstract Self-supervised learning (SSL) achieves great suc- cess in speech recognition, while limited explo- ration has been attempted for other speech pro- cessing tasks. As speech signal contains multi- faceted information including speaker identity, paralinguistics, spoken content, etc., learning uni- versal representations for all speech tasks is chal- lenging. In this paper, we propose a new pre- trained model, WavLM, to solve full-stack down- stream speech tasks. WavLM is built based on the HuBERT framework, with an emphasis on both spoken content modeling and speaker iden- tity preservation. We first equip the Transformer structure with gated relative position bias to im- prove its capability on recognition tasks. For bet- ter speaker discrimination, we propose an utter- ance mixing training strategy, where additional overlapped utterances are created unsupervisely and incorporated during model training. Lastly, we scale up the training dataset from 60k hours to 94k hours of public audio data, and optimize its training procedure for better representation extrac- tion. WavLM Large achieves state-of-the-art per- formance on the SUPERB benchmark, and brings significant improvements for various speech pro- cessing tasks on their representative benchmarks. Index TermsÃ¢ÂÂ Self-Supervised Learning, Speech Pre- Training. Introduction Over the past few years, self-supervised learning (SSL) has achieved great success in the fields of natural language pro- cessing (NLP) (Devlin et al., 2019; Yang et al., 2019; Raffel et al., 2020). It leverages large amounts of text data to learn universal text representations, which can benefit almost all NLP downstream tasks by fine-tuning. Recently, SSL has also shown prominent results for speech processing, espe- cially on phoneme classification (van den Oord et al., 2018) and automatic speech recognition (ASR) (Baevski et al., 2020b; Hsu et al., 2021a; Wang et al., 2021b). However, in other speech tasks, it is still standard practice to train models from scratch with task-specific datasets. Building a general pre-trained model can be essential to the further development of speech processing, because it can utilize large-scale unlabeled data to boost the performance in downstream tasks, reducing data labeling efforts, and lowering entry barriers for individual tasks. In the past, it has been infeasible to build such a general model, as dif- ferent tasks focus on different aspects of speech signals. For instance, speaker verification requires the network to learn the speaker characteristic regardless of the spoken con- tent, while speech recognition demands the network discard speaker characteristics and focus only on content informa- tion. Meanwhile, unlike verification and recognition tasks, speaker diarization and speech separation involve multiple speakers, which creates additional obstacles for learning general speech representations. Recent advances fueled by large-scale pre-trained models have changed the situation. Yang et al. (2021) proves the potential of pre-trained models on full-stack speech tasks by using the weighted sum of em- beddings from different layers.1 They find different layers contain information useful for different tasks. For instance, the hidden states of the top layers are useful for ASR, while the bottom layers are more effective for speaker verification. While exciting as a proof of concept, there are still some drawbacks in existing pre-trained models: 1) Current pre- trained models are unsatisfactory for multi-speaker tasks, such as speaker diarization and speech separation. Our ex- periments show that speech separation models trained on top of HuBERT (Hsu et al., 2021a), a top performed speech pre-trained model, achieve only marginal improvement compared with the models trained from scratch. This is mainly because the pre-training methods do not sufficiently enforce the speaker discrimination, and the training data contain only single-speaker audios. 2) Speech pre-training crucially relies on high quality and large quantities of unlabeled au- dios. The existing system utilizes Libri-Light (Kahn et al., 2020) as the main source, but the massive audiobook data mismatches the data in a real scenario and using it exclu- sively hurts the model performance when the acoustic char- acteristics of the downstream tasks are different from those of the audiobook. Hsu et al. (2021b) train wav2vec 2.0 (Baevski et al., 2020b) on larger and more diverse datasets, but there are still over 90%data derived from au- diobook. To eliminate the audiobook data bias, we try to gather data from different sources as much as possible in our experiments. In this paper, we present WavLM which learns universal speech representations from massive unlabeled speech data and adapts effectively across various speech processing tasks. WavLM is built based on the pre-training strategy of HuBERT, with three extensions for better speech char- acteristic modeling. 1) We add gated relative position bias (grep) (Chi et al., 2021) to the Transformer structure as the backbone, which improves model performance for ASR and keeps almost the same parameter number and training speed. Compared with the convolutional relative position embedding used in wav2vec 2.0 and HuBERT, the gates allow the relative position bias to be adjusted adaptively by conditioning on the current speech content. 2) To handle multi-speaker tasks, such as speaker diarization and speech separation, we propose an utterance-mixing training strat- egy, where partially overlapped signals are constructed to augment the training data, by mixing individual training samples with randomly selected speech pieces. 3) To fur- ther improve the model robustness and alleviate the data mismatch, we scale up unlabeled pre-training data to 94k hours of public audios. The dataset consists of 60k hours of Libri-Light, 10k hours of GigaSpeech (Chen et al., 2021a), and 24k hours of VoxPopuli (Wang et al., 2021a). The new dataset consists of training instances from different scenar- ios, such as podcasts, YouTube and European Parliament (EP) event recordings. We evaluate our models on 14 subtasks, ten of which are from SUPERB, and the other four are classic speech tasks on their representative testsets. WavLM achieves state-of-the-art performance on SU- PERB (Yang et al., 2021). The leaderboard screenshot is shown in Appendix A.4. WavLM Large outperforms HuBERT Large on all subtasks, and achieves an abso- lute 2.4 point improvement in overall evaluation. Even WavLM Base+, a 3 times smaller model, is better than HuBERT Large owing to our three modifications. Speaker verification is a task to verify the speaker identity from the voice characteristics. We select this task to evaluate the model capability of extracting speaker-related features. WavLM Large exceeds the well-known state-of-the-art (SOTA) system, ECAPA- TDNN (Desplanques et al., 2020), by a large mar- gin and achieves 0.431%, 0.538% and 1.154% EER (Equal Error Rate) on the three official trial lists of VoxCeleb1 (Nagrani et al., 2017). Speech separation is a classic multi-speaker task, which is the key to solving the cocktail party prob- lem. The task can evaluate the model capability of extracting multiple speech signals from a mixture of sounds. WavLM achieves SOTA performance on the speech separation LibriCSS benchmark (Chen et al., 2020), and significantly outperforms the previous Con- former model (Chen et al., 2021b) by a 27.7% relative word error rate (WER) reduction. SpeakerdiarizationisatasktorecognizeÃ¢ÂÂwhospoke whenÃ¢ÂÂ from an input audio stream (Park et al., 2021). WavLM achieves SOTA performance on the CALL- HOME speaker diarization benchmark. Compared to the EEND-EDA clustering method (Horiguchi et al., 2021b), our model achieves a 12.6% diarization error rate reduction. Speech recognition requires the model to learn con- tent information, which is the main focus of the pre- vious SSL work. We evaluate our model in the Lib- riSpeech 960h setting. WavLM shows comparable performance to the wav2vec 2.0 and HuBERT, which achieves 1.8% and 3.2% WER on the test-clean and test-other sets, respectively. The contribution of the paper can be summarized as follows: 1) WavLM sheds light on a general pre-trained model for full stack speech processing tasks, in contrast to the pre- vious SSL works focusing on a group of similar tasks. 2) We propose simple but effective modifications to the exist- ing pre-trained models, which show general and consistentacross downstream tasks. 3) We scale-up self-supervised speech pre-training with more unlabeled data and longer training steps. 4) We achieve state-of-the- art results on the SUPERB benchmark, and significantly boost the performance for various speech processing tasks on their representative benchmarks, including speech separation, speaker verification, and speaker diarization. The pre-training methods used in the models are effective for various speech processing tasks, including speaker diarization and speech separation. We evaluate our models on 14 subtasks, ten of which are from SUPERB, and the other four are classic speech tasks on their representative testsets. WavLM achieves state-of-the-art performance on SU- PERB (Supervised Learning Basecamp), and brings significant improvements for various speech pro- cessing tasks on their representative benchmarks. Index TermsÃ¢ÂÂ Self-Supervised Learning, Speech Pre- Training. Introduction Over the past few years, self-supervised learning (SSL) has achieved great success in the fields of natural language pro- cessing (NLP) (Devlin et al., 2019; Yang et al., 2019; Raffel et al., 2020). It leverages large amounts of text data to learn universal text representations, which can benefit almost all NLP downstream tasks by fine-tuning. Recently, SSL has also shown prominent results for speech processing, espe- cially on phoneme classification (van den Oord et al., 2018) and automatic speech recognition (ASR) (Baevski et al., 2020b; Hsu et al., 2021a; Wang et al., 2021b). However, in other speech tasks, it is still standard practice to train models from scratch with task-specific datasets. 
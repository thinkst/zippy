Fairness is a principal social value that can be observed in civilisations around the world. A manifestations of this is in social agreements, often described in texts, such as contracts. 
Yet, despite the prevalence of such, a fairness metric for texts describing a social act remains wanting. To address this, we take a step back to consider the problem based on first principals. 
Instead of using rules or templates, we utilise social psychology literature to determine the principal factors that humans use when making a fairness assessment. We then attempt to digitise 
these using word embeddings into a multi-dimensioned sentence level fairness perceptions vector to serve as an approximation for these fairness perceptions. The method leverages a pro-social 
bias within word embeddings, for which we obtain an F1= 81.0. A second approach, using PCA and ML based on the said fairness approximation vector produces an F1 score of 86.2. We details 
improvements that can be made in the methodology to incorporate the projection of sentence embedding on to a subspace representation of fairness. Keywords: Text analysis, NLP, Fairness, 
Social Cognition, Psychometrics Introduction A lay person reading a document, which describes a person acting with malice towards another is able to predict that a conflict is likely due 
to the presence of an unfair act. This judgment has its roots in both social and biological factors owing to humans being a highly evolved social species (Tomasello 2014). We ask, is it 
possible to approximate these perceptions, and incorporate them into a form of measurement tool? One that would allow a sentence describing the interactions of two or more individuals to be 
approximated as being fair or unfair? While natural language processing (NLP) for textual analysis has been used across a large number of disparate domains such as personality detection 
(Youyou, Kosinski, and Stillwell 2015), healthcare (Gaudet-Blavignac et al. 2021; Le Glaz et al. 2021, 202; 2021), product reviews (Paruchuri et al. 2021) and dialect detection (Al Shamsi 
and Abdallah 2021), a search of the literature for a measure that is able to detect how fair a sentence is yields no results. Much of the work on the topic of fairness and machine learning (ML) 
has focused on longstanding challenges surrounding algorithmic fairness (Hellman 2020), de-biasing word embeddings (Sun et al. 2019; Badilla, Bravo-Marquez, and PÃÂ©rez 2020), the use of 
counterfactuals (Corbett-Davies and Goel 2018; Chouldechova and Roth 2020), and problems associated with under representative data sets (Du et al. 2020). On the associated topic of morality, 
a number of papers have investigated the use of Moral Foundation Theory (MFT) (Graham et al. 2013) to analyse texts. MFT uses six foundations to parsimoniously explain the origins of human 
moral reasoning. These are Care/Harm, Fairness/Cheating, Loyalty/Betrayal, Authority/Subversion, Sanctity/Degradation, and Liberty/Oppression. The general approach has been to label a dataset of 
texts with these categories, and then apply a ML algorithm, e.g., long short-term memory (LSTM), support-vector machines (SVM), to learn the distinctions between each category (Rezapour, Shah, 
and Diesner 2019; Hoover et al. 2020; Rezapour, Dinh, and Diesner 2021). A similar approach has also been used based on pre-defined measures of moral language (Pennebaker, Francis, and Booth 2001; 
Araque, Gatti, and Kalimeri 2020). These approaches have proved useful as a form of topic modelling of language, yet they are unable to mark a sentence as being fair or unfair, or offer a degree 
of explainability as two why a classification was made, and to what degree each sentence is fair or unfair. Indeed a common challenge to ML systems of this type is that of limitations imposed by 
the technology on explainability (Danilevsky et al. 2020). This also presents a problem in work done using black-box ML to class documents as belonging to legally fair or unfair categories based 
on pre-trained legal clauses (Ruggeri et al. 2021; Lagioia et al. 2019). Their approach does not seek to identify explainable factors behind why something is perceived as fair or unfair, but instead, 
accept it as given that certain legal clauses are fair and others unfair,which they implement ML methods to draw a classification boundary. Testing their API (CLAUDETTE 2021) with the 
sentence "the boy will hit the girl", for example, produces the result: "Claudette found no potentially unfair clause". Which may be a reflection of out-of-domain knowledge limitations. Work done 
by (Schramowski et al. 2021) and (Jentzsch et al. 2021) have demonstrated that language models (LM) such as Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al. 2021) 
hold implicit representations of moral values. They do so using vector comparisons based on a template of DoÃ¢ÂÂs and DonÃ¢ÂÂts. Furthermore, a second approach, using BFTs (Schramowski et al. 2021) 
and ML based on the said fairness template produces an F1 score of 86.2. We details improvements that can be made in the methodology to incorporate the projection of sentence embedding on to a 
subspace representation of fairness. Keywords: Text analysis, NLP, Fairness, Social Cognition, Psychometrics Introduction A lay person reading a document, which describes a person acting with 
malice towards another is able to predict that a conflict is likely due to the presence of an unfair act. This judgment has its roots in both social and biological factors owing to humans being 
a highly evolved social species (Tomasello 2014). We ask, is it possible to approximate these perceptions, and incorporate them into a form of measurement tool? One that would allow a sentence 
describing the interactions of two or more individuals to be approximated as being fair or unfair? While natural language processing (NLP) for textual analysis has been used across a large number 
of disparate domains such as personality detection (Youyou, Kosinski, and Stillwell 2015), healthcare (Gaudet-Blavignac et al. 2021), product reviews (Paruchuri et al. 2021) and dialect detection 
(Al Shamsi and Abdallah 2021), a search of the literature for a measure that is able to detect how fair a sentence is yields no results. Much of the work on the topic of fairness and machine 
learning (ML) has focused on longstanding challenges surrounding algorithmic fairness (Hellman 2020), de-biasing word embeddings (Sun et al. 2019; Badilla, Bravo-Marquez, and Parez 2020), the 
use of counterfactuals (Corbett-Davies and Goel 2018; Chouldechova and Roth 2020), and problems associated with under representative data sets (Du et al. 2020). On the associated topic of morality, 
a number of papers have investigated the use of Moral Foundation Theory (MFT) (Graham et al. 2013) to analyse texts. MFT uses six foundations to parsimoniously explain the origins of human moral 
reasoning. These are Care/Harm, Fairness/Cheating, Loyalty/Betrayal, Authority/Subversion, Sanctity/Degradation, and Liberty/Oppression. The general approach has been to label a dataset of texts 
with these categories, and then apply a ML algorithm, e.g., long short-term memory (LSTM), support-vector machines (SVM), to learn the distinctions between each category (Rezapour, Shah, and 
Diesner 2019; Hoover et al. 2020; Rezapour, Dinh, and Diesner 2021). A similar approach has also been used based on pre-defined measures of moral language (Pennebaker, Francis, and Booth 2001; 
Araque, Gatti, and Kalimeri 2020). These approaches have proved useful as a form of topic modelling of language, yet they are unable to mark a sentence as being fair or unfair, or offer a degree 
of explainability as two why a classification was made, and to what degree each sentence is fair or unfair. Indeed a common challenge to ML systems of this type is that of limitations imposed by 
the technology on explainability (Danilevsky et al. 2020).
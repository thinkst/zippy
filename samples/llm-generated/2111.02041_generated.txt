Automatic spoken instruction understanding (SIU) of the controller-pilot conversations in the air traffic control (ATC) requires not only recognizing the words and semantics of the speech but also determining the role of the speaker. However, few of the published works on the automatic understanding systems in air traffic communication focus on speaker role identification (SRI). In this paper, we formulate the SRI task of controller-pilot communication as a binary classification problem. Furthermore, the text-based, speech-based, and speech and text based multi-modal methods are proposed to achieve a comprehensive comparison of the SRI task. To ablate the impacts of the comparative approaches, various advanced neural network architectures are applied to optimize the implementation of text-based and speech-based methods. Most importantly, a multi-modal speaker role identification network (MMSRINet) is designed to achieve the SRI task by considering both the speech and textual modality features. To aggregate modality features, the modal fusion module is proposed to fuse and squeeze acoustic and textual representations by modal attention mechanism and self-attention pooling layer, respectively. Finally, the comparative approaches are validated on the ATCSpeech corpus collected from a real-world ATC environment. The experimental results demonstrate that all the comparative approaches are worked for the SRI task, and the proposed MMSRINet shows the competitive performance and robustness than the other methods on both seen and unseen data, achieving 98.56%, and 98.08% accuracy, respectively. CCS Concepts: Ã¢ÂÂ¢ Computing methodologies Ã¢ÂÂ Information extraction. Additional Key Words and Phrases: speaker role identification, air traffic control, text classification, speech classification, spoken instruction understanding, multi-modal learning INTRODUCTION Speech communication between air traffic controllers (ATCOs) and pilots is one of the most important interaction ways in air traffic control (ATC) procedures. Recently, there is increasing interest in introducing automatic spoken instruction understanding (SIU) techniques to empower the ATC process [Lin 2021; Pardo et al. 2011]. In the past decades, it has been extensively investigated and widely applied on the safety detection system [Lin et al. 2020, 2019], the ATCOs training devices [SmiÃÂdl et al. 2019], the ATC assistance systems [Lin et al. 2021b; Schulder et al. 2015]. In practice, ATC speech communication can be regarded as task-oriented conversations between ATCOs and pilots. Thus, the SIU system of ATC is usually a pipeline that consists of several subtasks, i.e., speech activity detection (SAD), automatic speech recognition (ASR), text instruction understanding (TIU), and speaker role identification (SRI). In this pipeline, firstly, the speech segment is captured from the real-time streaming by the SAD module, and then the ASR system translates it into human-readable texts. Subsequently, the TIU module converts the natural texts into predefined structured instructions that are further processed by the computer. Finally, the computer-readable instructions and the speaker role (ATCO or pilot which was output by the SRI module) jointly provide a conversation context for other downstream applications. As can be seen from mentioned illustrations, the SRI module is a critical component of the SIU system in the field of ATC. However, most of the existing research of the ATC SIU systems focuses on the ASR and TIU techniques [Lin et al. 2021a,c; Oualil et al. 2017; Zuluaga-Gomez et al. 2020], and no detailed description of the SRI task was presented. A instruction understanding model and ATC communication rule-based methods for the SRI tasks were studied in [Lin et al. 2019], without providing SRI performance. To the best of our knowledge, none of the published works have reported complete approaches and results for the SRI tasks in the ATC domain. Since the ATCO communicates with several pilots by radio in a single frequency, the role of the speaker cannot be distinguished from the communication data link. However, the speaker role is a kind of indispensable and important information in many ATC-related applications, such as safety detection systems, ATCO workload analysis systems. Therefore, the inability to identify the speaker role directly from communication brings a certain challenge to the ATC-related SIU tasks. Fortunately, there are two kinds of data that can be served as the potential entities for the SRI tasks. Ã¢ÂÂ¢ Text: On the one hand, according to the communication rules recommended by the interna- tional civil aviation organization (ICAO),ATCOs should declare the call sign of the target aircraft before issuing the instructions, while the pilots read back the instructions firstly and then reporting their call sign. In general, most of the controller-pilot speech communication follow these rules, allowing the text classification to be a promising technology for SRI tasks. Ã¢ÂÂ¢ Speech: On the other hand, the speech can be considered as a representation of the speaker role from two aspects of signal and text. a) the controller-pilot speech communication presents distinctive features depending on the equipment and environment, such as a microphone, push-to-talk (PTT), background noise, radio. b) It implies the representation of its transcripts, which further provides more discriminative knowledge for the SRI task. In this paper, we define the SRI task as a binary classification problem, i.e., all the instructions are classified into two classes: ATCO or pilot. Meanwhile, the SRI task is addressed by the data- driven approaches from three different inputs, i.e., text, speech, speech-text. To this end, the text classification, audio classification, and multi-modal classification approaches are proposed to achieve the SRI task. In this procedure, several popular network architectures are introduced to serve as backbone networks for each approach to eliminate the impact of the difference between network architectures. The BiLSTM [Zhou et al. 2016], TextCNN [Kim 2014], and Transformer [Vaswani et al. 2017] architecture are developed as the backbone network in the text-based methods, while x-vector [Snyder et al. 2018], SincNet [Ravanelli and Bengio 2018a], and CRNN [Choi et al. 2017] architecture are built for the speech-based methods. Most importantly, a multi-modal speaker role identification network (MMSRINet) is designed to learn the distinctive representations from both the speech and textual modalities for the speech-text based methods. Specifically, a modal attention mechanism is proposed to fuse the different representations to a joint feature vector. In addition, the self-attention pooling layer is applied to produce the joint vector by the weighted sum operations, which further be regarded as the multi-modal embedding. Finally, the multi-modal embedding is further fed into classification layers to generate the final probabilities of the speaker role. All the proposed methods were validated on the ATCSpeech corpus [Yang et al. 2020] that was collected from a real-world ATC environment. In addition, in order to analyze and compare the performance and robustness of the model, we evaluate the trained model in two ways: 1) The model is validated on the test set of the ATCSpeech to evaluate the performance on the seen samples. 2) A supplement test set called test-s is used to verify the robustness of the model on the unseen samples of in controller-pilot communication. In summary, our contributions are listed as follows: Ã¢ÂÂ¢ A thorough comparison between the aforementioned deep learning based SRI techniques is investigated. To the best of our knowledge, this is the first work that investigates the SRI task in the ATC domain. Ã¢ÂÂ¢ The robustness and performance of the comparative models are comprehensively analyzed and discussed on the seen and unseen samples. Ã¢ÂÂ¢ A multi-modal model, called MMSRINet, is proposed to achieve the ATC-related SRI task by considering both the speech and textual modal knowledge, which shows more competitive performance and robustness than other methods. This paper is organized as follows. A brief review of related works is described in Section 2. Section 3 presents the detail of the proposed methods and architectures. The experimental configurations are provided in Section 4. The experimental results are reported and detailly discussed in Section 5. Finally, this paper is concluded in Section 6. RELATED WORK 2.1 Text Classification Text classification is a classical task in the field of natural language processing (NLP), which aims to classify a given text sequences into a certain class. In general, the approach can be grouped into two categories: rule-based methods and data-driven based methods. The rule-based approach usually requires a large number of predefined rules and is strongly dependent on domain knowledge, which can only be applied to limited scenarios due to poor flexibility. Thanks to the development of deep learning techniques, the performance of data-driven meth- ods has generally outperformed that of rule-based methods in recent years and has become the standard paradigm of text classification tasks [Minaee et2021]. Zeng et al. [Kim 2014] utilized a convolutional neural network (CNN) [Lecun et al. 1998] to achieve the sentence classification tasks which makes representative progress in the NLP domain. To capture the long-term depen- dencies, the Att-BiLSTM model [Zhou et al. 2016] was built on a recurrent neural network (RNN) [Mikolov et al. 2010]. Currently, various improved methods based on the CNN or RNN block were proposed to achieve the text classification task, such as Character-level CNNs [Zhang et al. 2015], tree-based CNN [Mou et al. 2016], Tree-LSTM [Tai et al. 2015], Multi-Timescale LSTM [Liu et al. 2015]. With the successful applications of the Transformer architecture [Vaswani et al. 2017], many Transformer-based and pre-trained language models were also proposed and achieved surprising performance [Devlin et al. 2019; Radford et al. 2018]. These methods achieved new state-of-the-art performance in text classification tasks by fine-tuning the pre-trained models [Sun et al. 2019]. 2.2 Audio Classification Audio classification is widely applied in audio pattern recognition tasks, such as speaker identifica- tion [Ravanelli and Bengio 2018a; Snyder et al. 2018], acoustic event detection [Kumar and Raj 2016], accent classification [Hansen and Liu 2016; Lopez-Moreno et al. 2014], audio emotion recognition [Jermsittiparsert et al. 2020]. Recently, deep learning methods showed promising performance compared to traditional approaches for this task [Hershey et al. 2017]. Enormous works have been investigated to explore different model architectures and applications for audio classification. Shawn Hershey et al. [Hershey et al. 2017] demonstrated that the CNNs used in the image clas- sification task, such as AlexNet [Krizhevsky et al. 2012], VGG [Simonyan and Zisserman 2014], and ResNet [He et al. 2016], achieved desired performance for the large-scale audio classification task. Meanwhile, the convolutional recurrent neural network (CRNN) was proposed and to achieve music classification [Choi et al. 2017], audio event detection [CakÃÂ±r et al. 2017], audio tagging [Xu et al. 2018], etc. In addition, in recent years, there are increasing interest in learning features from raw waveforms directly instead of handcraft features. Mirco Ravanelli et al. proposed the SincNet [Ravanelli and Bengio 2018a] to achieve speaker recognition which employs band-pass filters (based on the parametrized Sinc functions) in the first convolutional layer. Jee-weon Jung et al. proposed the RawNet [Jung et al. 2019] to improve the performance of the speaker verification from raw waveforms. In short, deep learning-based audio classification is still an interesting task in many applications. 2.3 Multi-modal Classification With the explosive growth of multi-modal data in the digit world, multi-modal learning is attracting increasing research interest and shows powerful performance than that of unimodal modal methods [Ngiam et al. 2011]. Various modalities can be used to achieve classification tasks, including audio- video [Nagrani et al. 2018], audio-text [Mittal et al. 2020], image-text [Gallo et al. 2018; Kiela et al. 2018], etc. In general, the fusion strategy of the classification task can be implemented in the following two ways: early fusion and later fusion [BaltrusÃÂaitis et al. 2019]. Early fusion methods fuse the multi-modal feature vectors to a joint representation that is further fed into the classifier, while the later fusion makes a second decision on the output of two classifiers by an extra strategy. Due to the advantages of early fusion in exploring the correlations and interactions between different modalities, in this paper, we introduce the early fusion methods to the SRI task. In an early work [Kiela and Bottou 2014], direct concatenation was employed to produce multi-modal joint representations. In order to identify the correlations of learned multi-modal features, a structural regularization was proposed in [Wu et al. 2014] to empower the deep neural network (DNN) based fusion layer, which also preserves the diversitythe different modality features. In addition to the classification task, more powerful fusion methods were successfully integrated into the ASR and NLP architectures. Modality attention was proposed to fuse the audio-visual features for ASR tasks in [Zhou et al. 2019]. The works of [Fukui et al. 2016] and [Ovalle et al. 2017] used deep learning [Xu et al. 2017] to achieve the SRI task. In addition, in recent years, there are increasing interest in introducing automatic spoken instruction understanding (SIU) techniques to empower the ATC process [Zhou et al. 2019]. In the past decades, it has been extensively investigated and widely applied on the safety detection system [Lin et al. 2020, 2019], the ATCOs training devices [SmiÃÂdl et al. 2019], the ATC assistance systems [Lin et al. 2021b; Schulder et al. 2015]. In practice, ATC speech communication can be regarded as task-oriented conversations between ATCOs and pilots. Thus, the SIU system of ATC is usually a pipeline that consists of several subtasks, i.e., speech activity detection (SAD), automatic speech recognition (ASR), text instruction understanding (TIU), and speaker role identification (SRI). In this pipeline, firstly, the speech segment is captured from the real-time streaming by the SAD module, and then the ASR system translates it into human-readable texts. 